# -*- coding: utf-8 -*-
"""trainMobileNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wjSpQ7xqmx7RcekYKCHIRmJ2vpPlurve

### Mount Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Filepaths"""

noisy_train = "/content/drive/Shareddrives/VoxCeleb1/Dev_Augmented/wav"
clean_test = "/content/drive/Shareddrives/VoxCeleb1/Test/wav/wav"
noisy_test = "/content/drive/Shareddrives/VoxCeleb1/Test_Augmented/wav/wav"
filtered_test = "/content/drive/Shareddrives/VoxCeleb1/Test_Filtered/wav"
workspace = "/content/drive/Shareddrives/VoiceAuth"

"""### cd to working directory"""

# Commented out IPython magic to ensure Python compatibility.
# %cd $workspace

!export PYTHONPATH=$PWD:$PYTHONPATH

"""## Generate Verification Pairs"""

!python scripts/make_verification_pairs.py $noisy_train  data/pairs/pairs_raw_train.csv  --imposter_ratio 3 --seed 42
!python scripts/make_verification_pairs.py $clean_test  data/pairs/pairs_raw_clean_test.csv  --imposter_ratio 3 --seed 42
!python scripts/make_verification_pairs.py $noisy_test data/pairs/pairs_raw_noisy_test.csv  --imposter_ratio 3 --seed 42
!python scripts/make_verification_pairs.py $filtered_test data/pairs/pairs_raw_filtered_test.csv --imposter_ratio 3 --seed 42

"""## Generate Mel-Spectrogram Pairs"""

# TRAIN SPLIT → spectrograms/noisy_train/…
!python scripts/make_spectrogram_pairs.py \
  data/pairs/pairs_raw_train.csv \
  data/spectrograms/noisy_train/ \
  data/pairs/pairs_spec_train.csv \
  --img_format jpg \
  --quality 40 \
  --n_mels 32 \
  --img_size 112 \
  --duration 2.0

# CLEAN TEST → spectrograms/clean_test/…
!python scripts/make_spectrogram_pairs.py \
  data/pairs/pairs_raw_clean_test.csv \
  data/spectrograms/clean_test/ \
  data/pairs/pairs_spec_clean_test.csv \
  --img_format jpg \
  --quality 40 \
  --n_mels 32 \
  --img_size 112 \
  --duration 2.0

"""## Train Embedding Extractors

### Train MobileNetV2 embedding extractor
"""

import csv
import os
import shutil
from pathlib import Path
from tqdm.notebook import tqdm # Use tqdm.notebook for Colab progress bars
import time

def copy_data_and_update_csv(drive_source_base_dir, local_dest_base_dir,
                             original_csv_path, local_csv_path,
                             data_type_name="Data",
                             # Assume CSV paths are relative to this workspace dir
                             drive_workspace_dir=Path("/content/drive/Shareddrives/VoiceAuth")):
    """
    Copies unique files listed in a CSV from a source directory to a local
    destination and creates a new CSV pointing to the local files.
    Handles paths in the CSV that might be relative to drive_workspace_dir.

    Args:
        drive_source_base_dir (Path): Base directory on the source (e.g., Drive)
                                      containing the specific dataset type (e.g., .../noisy_train/).
                                      Used to calculate relative paths for destination.
        local_dest_base_dir (Path): Base directory on the local Colab storage.
        original_csv_path (Path): Path to the input CSV with source paths.
        local_csv_path (Path): Path where the output CSV with local paths will be saved.
        data_type_name (str): Name of the data being processed (for print statements).
        drive_workspace_dir (Path): The root workspace directory on Drive, used to
                                    resolve relative paths found in the CSV.
    """
    print(f"--- Processing {data_type_name} ---")
    print(f"Drive Workspace Dir (for resolving relative paths): {drive_workspace_dir}")
    print(f"Source Base Dir (for relative dest path): {drive_source_base_dir}")
    print(f"Destination Base Dir: {local_dest_base_dir}")
    print(f"Original CSV: {original_csv_path}")
    print(f"Output CSV: {local_csv_path}")

    # --- Safety Checks ---
    if not drive_workspace_dir.exists():
         print(f"ERROR: Drive workspace directory not found: {drive_workspace_dir}")
         return False
    # drive_source_base_dir might be inside workspace, check it too if needed,
    # but existence check happens per-file later.
    if not original_csv_path.exists():
        print(f"ERROR: Original CSV file not found: {original_csv_path}")
        return False

    print("Workspace directory and original CSV found.")

    # --- Step 1: Collect unique FULL source file paths ---
    print(f"Reading {original_csv_path} and resolving paths...")
    unique_full_source_paths = set()
    original_pairs_data = [] # Store original relative paths + label for rewriting CSV
    source_paths_for_csv_rewrite = {} # Map full_src_path -> original_relative_path

    try:
        with open(original_csv_path, 'r', newline='') as infile:
            reader = csv.reader(infile)
            header = next(reader)
            original_pairs_data.append(header) # Keep header
            row_count = 0
            skipped_rows = 0
            for row in reader:
                row_count += 1
                if len(row) == 3:
                    f1_rel, f2_rel, label = row # These are likely relative paths

                    # Construct full paths by joining workspace with relative path
                    full_src_path1 = (drive_workspace_dir / f1_rel).resolve()
                    full_src_path2 = (drive_workspace_dir / f2_rel).resolve()

                    # Check if the *resolved full paths* exist
                    path1_exists = full_src_path1.exists()
                    path2_exists = full_src_path2.exists()

                    if path1_exists and path2_exists:
                        unique_full_source_paths.add(full_src_path1)
                        unique_full_source_paths.add(full_src_path2)
                        # Store original relative paths for rewriting the new CSV later
                        original_pairs_data.append([f1_rel, f2_rel, label])
                        # Store mapping for easy lookup during CSV rewrite
                        source_paths_for_csv_rewrite[full_src_path1] = f1_rel
                        source_paths_for_csv_rewrite[full_src_path2] = f2_rel
                    else:
                        skipped_rows += 1
                        if not path1_exists:
                             print(f"Warning: Resolved source file does not exist: {full_src_path1} (from row {row_count+1}, path '{f1_rel}')")
                        if not path2_exists:
                             print(f"Warning: Resolved source file does not exist: {full_src_path2} (from row {row_count+1}, path '{f2_rel}')")
                        # Skip adding this row to original_pairs_data if files don't exist
                else:
                     print(f"Warning: Skipping malformed row {row_count+1} in CSV: {row}")
                     skipped_rows += 1

            if skipped_rows > 0:
                 print(f"Skipped {skipped_rows} rows due to missing files or formatting issues.")

    except FileNotFoundError:
         print(f"ERROR: Could not find {original_csv_path}")
         return False
    except Exception as e:
        print(f"ERROR: Failed to read {original_csv_path}: {e}")
        return False

    print(f"Found {len(unique_full_source_paths)} unique existing source files to potentially copy.")
    if not unique_full_source_paths:
         print(f"Warning: No unique source files found for {data_type_name}. Cannot proceed with copy.")
         # Attempt to write empty CSV? Or return False? Let's return False.
         return False

    # --- Step 2: Copy unique files ---
    print(f"Starting copy process for {data_type_name}...")
    start_time = time.time()
    copied_count = 0
    skipped_count = 0

    local_dest_base_dir.mkdir(parents=True, exist_ok=True)
    valid_full_paths_for_csv_rewrite = set() # Track full source paths successfully copied/found

    for full_src_path in tqdm(list(unique_full_source_paths), desc=f"Copying {data_type_name} files", unit="file"):
        try:
            # Calculate path relative to the *specific dataset source base*
            # (e.g., .../noisy_train/) for correct destination structure
            relative_path_for_dest = full_src_path.relative_to(drive_source_base_dir)
            dest_file_path = local_dest_base_dir / relative_path_for_dest

            dest_file_path.parent.mkdir(parents=True, exist_ok=True)

            if not dest_file_path.exists():
                 # Source existence already checked, but double-check just in case
                 if full_src_path.exists():
                    shutil.copy2(full_src_path, dest_file_path)
                    copied_count += 1
                    valid_full_paths_for_csv_rewrite.add(full_src_path) # Mark as valid
                 else:
                     print(f"\nWarning: Source file {full_src_path} disappeared before copy. Skipping.")
            else:
                skipped_count += 1
                valid_full_paths_for_csv_rewrite.add(full_src_path) # Also valid if already exists

        except ValueError as e:
             # This happens if full_src_path is not relative to drive_source_base_dir
             print(f"\nWarning: Error calculating relative path for destination for file '{full_src_path}' relative to base '{drive_source_base_dir}'. Skipping copy. Error: {e}")
        except Exception as e:
            print(f"\nERROR: Failed to copy {full_src_path} to {dest_file_path}: {e}")

    end_time = time.time()
    print(f"\nCopy process finished for {data_type_name}.")
    print(f" - Files copied: {copied_count}")
    print(f" - Files skipped (already existed): {skipped_count}")
    print(f" - Time taken: {end_time - start_time:.2f} seconds")

    # # --- Step 3: Create the new CSV ---
    # print(f"Creating new CSV with local paths at: {local_csv_path}...")
    # rewritten_rows = 0
    # try:
    #     local_csv_path.parent.mkdir(parents=True, exist_ok=True)
    #     with open(local_csv_path, 'w', newline='') as outfile:
    #         writer = csv.writer(outfile)
    #         writer.writerow(original_pairs_data[0]) # Write header

    #         # Iterate through the original pairs data (which now only contains rows for existing files)
    #         for row_data in tqdm(original_pairs_data[1:], desc=f"Writing {data_type_name} CSV", unit="row"):
    #             if len(row_data) == 3:
    #                 f1_rel, f2_rel, label = row_data

    #                 # Reconstruct full source paths to check if they were valid
    #                 full_src_path1 = (drive_workspace_dir / f1_rel).resolve()
    #                 full_src_path2 = (drive_workspace_dir / f2_rel).resolve()

    #                 # Only write the row if BOTH original files were successfully copied or existed locally
    #                 if full_src_path1 in valid_full_paths_for_csv_rewrite and full_src_path2 in valid_full_paths_for_csv_rewrite:
    #                     try:
    #                         # Calculate paths relative to the specific dataset source base dir
    #                         rel1_for_dest = full_src_path1.relative_to(drive_source_base_dir)
    #                         rel2_for_dest = full_src_path2.relative_to(drive_source_base_dir)

    #                         # Construct final local paths
    #                         f1_local = str(local_dest_base_dir / rel1_for_dest)
    #                         f2_local = str(local_dest_base_dir / rel2_for_dest)

    #                         writer.writerow([f1_local, f2_local, label])
    #                         rewritten_rows += 1
    #                     except ValueError as e:
    #                          print(f"\nWarning: Error calculating relative path for pair ({f1_rel}, {f2_rel}) when writing CSV. Skipping row. Error: {e}")
    #                     except Exception as e:
    #                          print(f"\nError processing row for new CSV ({f1_rel}, {f2_rel}): {e}")
    #                 # else: # Optional: Debugging if rows are skipped during rewrite
    #                 #    print(f"Skipping row ({f1_rel}, {f2_rel}) during CSV rewrite as one/both files were not validated.")


    #     print(f"Successfully created new CSV: {local_csv_path} with {rewritten_rows} data rows.")
    #     if rewritten_rows == 0 and len(original_pairs_data) > 1:
    #          print(f"Warning: The output CSV for {data_type_name} contains no data rows. Check source paths and copy process.")
    #     print(f"--- Finished processing {data_type_name} ---")
    #     return True # Indicate success

    # except Exception as e:
    #     print(f"ERROR: Failed to create the new CSV file {local_csv_path}: {e}")
    #     print(f"--- Finished processing {data_type_name} (with errors) ---")
    #     return False # Indicate failure

!python -m scripts.data_manager_v2 \
  --drive_workspace_dir="/content/drive/Shareddrives/VoiceAuth" \
  --copy_spec_train
  # --force_copy # Optional: Overwrite existing local JPG files

workspace_dir = Path("/content/drive/Shareddrives/VoiceAuth")
train_spec_drive_source_base = workspace_dir / "data/spectrograms/noisy_train/" # Check this carefully!

train_spec_local_dest_base = Path("/content/data/spectrograms/noisy_train/")
train_spec_original_csv = Path("data/pairs/pairs_spec_train.csv")
train_spec_local_csv = Path("data/pairs/pairs_spec_train_local.csv")

print(f"Copying Training Spectrograms...")
copy_data_and_update_csv(
    drive_source_base_dir=train_spec_drive_source_base,
    local_dest_base_dir=train_spec_local_dest_base,
    original_csv_path=train_spec_original_csv,
    local_csv_path=train_spec_local_csv,
    data_type_name="Training Spectrograms"
)
print("Done with Training Spectrograms cell.")



!python -m scripts.train_mobilenetv2 \
    data/pairs/pairs_spec_train_local.csv \
    --data_root /content/data/spectrograms/noisy_train/ \
    --epochs 50 --batch_size 128 --lr 1e-4 \
    --val_split 0.1 --patience 5 --seed 42 \
    --resume --output_dir outputs --device cuda \
    --num_workers=8 --freeze_features
# !python scripts/train_mobilenetv2.py \
#     data/pairs/pairs_spec_train.csv \
#     --data_root /content/data/spectrograms \
#     --epochs 50 --batch_size 32 --lr 1e-4 \
#     --val_split 0.1 --patience 5 --seed 42 \
#     --resume --output_dir outputs --device cuda

import csv
import os
import shutil
from pathlib import Path
from tqdm.notebook import tqdm # Use tqdm.notebook for Colab progress bars
import time

# --- Configuration ---

# 1. Source directory on Google Drive (Ensure this matches your notebook variable)
#    From your notebook: noisy_train = "/content/drive/Shareddrives/VoxCeleb1/Dev_Augmented/wav"
drive_source_base_dir = Path("/content/drive/Shareddrives/VoxCeleb1/Dev_Augmented/wav")

# 2. Destination directory on Colab's local storage
local_dest_base_dir = Path("/content/data/raw_audio/noisy_train")

# 3. Path to the original CSV file containing Drive paths
original_csv_path = Path("data/pairs/pairs_raw_train.csv")

# 4. Path for the new CSV file with local Colab paths
local_csv_path = Path("data/pairs/pairs_raw_train_local.csv")

# --- Safety Checks ---
if not drive_source_base_dir.exists():
    print(f"ERROR: Source directory on Google Drive not found: {drive_source_base_dir}")
    print("Please ensure your Drive is mounted and the path is correct.")
    #raise FileNotFoundError(f"Source directory not found: {drive_source_base_dir}") # Optional: stop execution
elif not original_csv_path.exists():
    print(f"ERROR: Original CSV file not found: {original_csv_path}")
    #raise FileNotFoundError(f"Original CSV not found: {original_csv_path}") # Optional: stop execution
else:
    print("Source directory and original CSV found. Proceeding...")

    # --- Step 1: Collect unique file paths from the original CSV ---
    print(f"Reading unique file paths from {original_csv_path}...")
    unique_source_files = set()
    original_pairs_data = [] # Store original pairs to write the new CSV later
    try:
        with open(original_csv_path, 'r', newline='') as infile:
            reader = csv.reader(infile)
            header = next(reader) # Skip header
            original_pairs_data.append(header) # Keep header for new CSV
            for row in reader:
                if len(row) == 3:
                    f1, f2, label = row
                    # Basic validation: check if path seems plausible (optional)
                    if str(drive_source_base_dir) in f1:
                         unique_source_files.add(Path(f1))
                    else:
                         print(f"Warning: File path '{f1}' in CSV doesn't seem to be under source base '{drive_source_base_dir}'. Skipping.")
                    if str(drive_source_base_dir) in f2:
                         unique_source_files.add(Path(f2))
                    else:
                         print(f"Warning: File path '{f2}' in CSV doesn't seem to be under source base '{drive_source_base_dir}'. Skipping.")

                    original_pairs_data.append(row) # Store row for rewriting
                else:
                     print(f"Warning: Skipping malformed row in CSV: {row}")

    except Exception as e:
        print(f"ERROR: Failed to read {original_csv_path}: {e}")
        # Consider adding 'raise e' here if you want execution to stop on error

    print(f"Found {len(unique_source_files)} unique source audio files to copy.")

    # --- Step 2: Copy unique files to local Colab storage ---
    print(f"Starting copy from {drive_source_base_dir} to {local_dest_base_dir}...")
    print("This may take a long time depending on dataset size and network speed.")
    start_time = time.time()
    copied_count = 0
    skipped_count = 0

    # Create the base destination directory if it doesn't exist
    local_dest_base_dir.mkdir(parents=True, exist_ok=True)

    for src_file_path in tqdm(list(unique_source_files), desc="Copying files"):
        try:
            # Calculate the path relative to the source base directory
            relative_path = src_file_path.relative_to(drive_source_base_dir)
            # Construct the full destination path
            dest_file_path = local_dest_base_dir / relative_path

            # Create the parent directory for the destination file if it doesn't exist
            dest_file_path.parent.mkdir(parents=True, exist_ok=True)

            # Copy the file only if it doesn't exist locally already
            if not dest_file_path.exists():
                shutil.copy2(src_file_path, dest_file_path) # copy2 preserves metadata like modification time
                copied_count += 1
            else:
                skipped_count += 1

        except ValueError as e:
             print(f"\nWarning: Skipping file '{src_file_path}' due to path issue (is it outside the base dir?): {e}")
        except Exception as e:
            print(f"\nERROR: Failed to copy {src_file_path} to {dest_file_path}: {e}")
            # Optionally, decide whether to continue or stop on error

    end_time = time.time()
    print(f"\nCopy process finished.")
    print(f" - Files copied: {copied_count}")
    print(f" - Files skipped (already existed): {skipped_count}")
    print(f" - Time taken: {end_time - start_time:.2f} seconds")

    # --- Step 3: Create the new CSV with local paths ---
    print(f"Creating new CSV with local paths at: {local_csv_path}...")
    try:
        local_csv_path.parent.mkdir(parents=True, exist_ok=True) # Ensure parent dir exists
        with open(local_csv_path, 'w', newline='') as outfile:
            writer = csv.writer(outfile)
            # Write header (already stored in original_pairs_data[0])
            writer.writerow(original_pairs_data[0])

            # Write rows with updated paths
            for row in tqdm(original_pairs_data[1:], desc="Writing new CSV"): # Skip header row
                 if len(row) == 3:
                    f1_drive, f2_drive, label = row
                    try:
                        # Recalculate local paths based on drive paths
                        rel1 = Path(f1_drive).relative_to(drive_source_base_dir)
                        f1_local = str(local_dest_base_dir / rel1)

                        rel2 = Path(f2_drive).relative_to(drive_source_base_dir)
                        f2_local = str(local_dest_base_dir / rel2)

                        writer.writerow([f1_local, f2_local, label])
                    except ValueError as e:
                        print(f"\nWarning: Skipping row in new CSV due to path issue for pair ({f1_drive}, {f2_drive}): {e}")
                    except Exception as e:
                         print(f"\nError processing row for new CSV ({f1_drive}, {f2_drive}): {e}")

                 else:
                      # This shouldn't happen if filtering worked earlier, but good to handle
                      print(f"Warning: Skipping malformed row data when writing new CSV: {row}")

        print(f"Successfully created new CSV: {local_csv_path}")
        print("\nIMPORTANT: Update your training script command to use this new CSV file!")
        print(f"Example: !python scripts/train_sincnet.py {local_csv_path} ... (other args)")

    except Exception as e:
        print(f"ERROR: Failed to create the new CSV file: {e}")

# --- Final Check ---
# Optional: Check disk usage
# !du -sh /content/data/raw_audio/noisy_train

"""## Train Fusion Classifier"""

!python -m scripts.train_fusion \
    data/pairs/pairs_raw_train_local.csv  data/pairs/pairs_spec_train_local.csv \
    --spec_data_root /content/data/spectrograms \
    --epochs 50 --batch_size 32 --lr 1e-4 \
    --val_split 0.1 --patience 5 --seed 42 \
    --resume --output_dir outputs --device cuda

!python -m scripts.analyze_results \
    --metrics_dir outputs/metrics \
    --output_dir outputs/figures

# NOISY TEST → spectrograms/noisy_test/…
!python -m scripts.make_spectrogram_pairs \
  data/pairs/pairs_raw_noisy_test.csv \
  data/spectrograms/noisy_test/ \
  data/pairs/pairs_spec_noisy_test.csv \
  --img_format jpg \
  --quality 40 \
  --n_mels 32 \
  --img_size 112 \
  --duration 2.0

# FILTERED TEST → spectrograms/filtered_test/…
!python -m scripts.make_spectrogram_pairs \
  data/pairs/pairs_raw_filtered_test.csv \
  data/spectrograms/filtered_test/ \
  data/pairs/pairs_spec_filtered_test.csv \
  --img_format jpg \
  --quality 40 \
  --n_mels 32 \
  --img_size 112 \
  --duration 2.0

# --- Cell for All Test Sets ---

# Define base paths from your notebook
drive_base = Path("/content/drive/Shareddrives/VoxCeleb1")
workspace_dir = Path("/content/drive/Shareddrives/VoiceAuth") # As defined before

# Base directories for RAW audio on Drive
raw_drive_bases = {
    "clean": drive_base / "Test/wav/wav",
    "noisy": drive_base / "Test_Augmented/wav/wav",
    "filtered": drive_base / "Test_Filtered/wav",
}

# Base directories for SPECTROGRAMS on Drive (relative to workspace)
# Derived from make_spectrogram_pairs.py calls in the notebook
spec_drive_bases = {
    "clean": workspace_dir / "data/spectrograms/clean_test/",
    "noisy": workspace_dir / "data/spectrograms/noisy_test/",
    "filtered": workspace_dir / "data/spectrograms/filtered_test/",
}

# Base directories for local storage in Colab
raw_local_bases = {
    "clean": Path("/content/data/raw_audio/clean_test/"),
    "noisy": Path("/content/data/raw_audio/noisy_test/"),
    "filtered": Path("/content/data/raw_audio/filtered_test/"),
}
spec_local_bases = {
    "clean": Path("/content/data/spectrograms/clean_test/"),
    "noisy": Path("/content/data/spectrograms/noisy_test/"),
    "filtered": Path("/content/data/spectrograms/filtered_test/"),
}

# Define CSV file paths
csv_files = {
    "raw": {
        "clean":    ("data/pairs/pairs_raw_clean_test.csv", "data/pairs/pairs_raw_clean_test_local.csv"),
        "noisy":    ("data/pairs/pairs_raw_noisy_test.csv", "data/pairs/pairs_raw_noisy_test_local.csv"),
        "filtered": ("data/pairs/pairs_raw_filtered_test.csv", "data/pairs/pairs_raw_filtered_test_local.csv"),
    },
    "spec": {
        "clean":    ("data/pairs/pairs_spec_clean_test.csv", "data/pairs/pairs_spec_clean_test_local.csv"),
        "noisy":    ("data/pairs/pairs_spec_noisy_test.csv", "data/pairs/pairs_spec_noisy_test_local.csv"),
        "filtered": ("data/pairs/pairs_spec_filtered_test.csv", "data/pairs/pairs_spec_filtered_test_local.csv"),
    }
}

splits = ["clean", "noisy", "filtered"]

# --- Copy Raw Test Audio ---
print("\n=== Processing Raw Test Audio ===")
for split in splits:
    print(f"\nProcessing split: {split}")
    orig_csv, local_csv = csv_files["raw"][split]
    copy_data_and_update_csv(
        drive_source_base_dir=raw_drive_bases[split],
        local_dest_base_dir=raw_local_bases[split],
        original_csv_path=Path(orig_csv),
        local_csv_path=Path(local_csv),
        data_type_name=f"Raw Test Audio ({split})"
    )

# --- Copy Spectrogram Test Images ---
print("\n=== Processing Spectrogram Test Images ===")
for split in splits:
    print(f"\nProcessing split: {split}")
    orig_csv, local_csv = csv_files["spec"][split]
    copy_data_and_update_csv(
        drive_source_base_dir=spec_drive_bases[split], # Ensure this base path is correct!
        local_dest_base_dir=spec_local_bases[split],
        original_csv_path=Path(orig_csv),
        local_csv_path=Path(local_csv),
        data_type_name=f"Spectrogram Test Images ({split})"
    )

print("\nDone with Test Sets cell.")
print("IMPORTANT: Update your evaluation script command ('evaluate.py')")
print("to use the new '_local.csv' files for each test split.")

# 6) Evaluate on clean / noisy / filtered
!python -m scripts.evaluate \
    outputs/checkpoints/fusion_best.pt \
    --test_splits clean   data/pairs/pairs_raw_clean_test_local.csv   data/pairs/pairs_spec_clean_test_local.csv \
                   noisy   data/pairs/pairs_raw_noisy_test_local.csv   data/pairs/pairs_spec_noisy_test_local.csv \
                   filtered data/pairs/pairs_raw_filtered_test_local.csv data/pairs/pairs_spec_filtered_test_local.csv \
    --spec_data_root /content/data/spectrograms \
    --batch_size 32 --device cuda --output_dir outputs